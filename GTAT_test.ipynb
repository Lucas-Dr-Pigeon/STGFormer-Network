{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f12cedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from typing import Optional, Tuple, Union\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, Size\n",
    "from torch_geometric.utils import (\n",
    "    add_self_loops,\n",
    "    remove_self_loops,\n",
    "    softmax,\n",
    ")\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "\n",
    "# PyG initialization helpers\n",
    "from torch_geometric.nn.inits import glorot, zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235f8240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTATConv(MessagePassing):\n",
    "    _alpha: OptTensor\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, heads: int,\n",
    "                 topology_channels:int = 15,\n",
    "                 concat: bool = True, negative_slope: float = 0.2,\n",
    "                 dropout: float = 0., add_self_loops: bool = True,\n",
    "                 bias: bool = True, share_weights: bool = False, **kwargs):\n",
    "        super(GTATConv, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.topology_channels = topology_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.share_weights = share_weights\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias,\n",
    "                            weight_initializer='glorot')\n",
    "        \n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias,\n",
    "                                weight_initializer='glorot')\n",
    "        \n",
    "        \n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        self.att2 = Parameter(torch.Tensor(1, heads, self.topology_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha1 = None\n",
    "        self._alpha2 = None\n",
    "\n",
    "        self.bias2 =  Parameter(torch.Tensor(self.topology_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "        glorot(self.att)\n",
    "        glorot(self.att2)\n",
    "        zeros(self.bias)\n",
    "        zeros(self.bias2)\n",
    "\n",
    "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
    "                topology: Tensor,\n",
    "                size: Size = None, return_attention_weights: bool = None):\n",
    "        # type: (Union[Tensor, PairTensor], Tensor , Tensor, Size, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, Size, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, Size, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, Size, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            return_attention_weights (bool, optional): If set to :obj:`True`,\n",
    "                will additionally return the tuple\n",
    "                :obj:`(edge_index, attention_weights)`, holding the computed\n",
    "                attention weights for each edge. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l: OptTensor = None\n",
    "        x_r: OptTensor = None\n",
    "        if isinstance(x, Tensor):\n",
    "            assert x.dim() == 2\n",
    "            x_l = self.lin_l(x).view(-1, H, C)  #(N , heads, features)\n",
    "            if self.share_weights:\n",
    "                x_r = x_l\n",
    "            else:\n",
    "                x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "        topology = topology.unsqueeze(dim = 1)\n",
    "        topology = topology.repeat(1, self.heads, 1)\n",
    "        x_l = torch.cat((x_l,topology), dim = -1)\n",
    "        x_r = torch.cat((x_r,topology), dim = -1)\n",
    "\n",
    "        if self.add_self_loops:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                num_nodes = x_l.size(0)\n",
    "                if x_r is not None:\n",
    "                    num_nodes = min(num_nodes, x_r.size(0))\n",
    "                if size is not None:\n",
    "                    num_nodes = min(size[0], size[1])\n",
    "                edge_index, _ = remove_self_loops(edge_index)\n",
    "                edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                edge_index = set_diag(edge_index)\n",
    "\n",
    "        out_all = self.propagate(edge_index, x=(x_l, x_r), size=size)\n",
    "        out = out_all[ : , : , :self.out_channels ]\n",
    "        out2 = out_all[ : , : , self.out_channels:]\n",
    "        alpha1 = self._alpha1\n",
    "        self._alpha1 = None\n",
    "        alpha2 = self._alpha2\n",
    "        self._alpha2 = None\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.reshape(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        out2 = out2.mean(dim=1)\n",
    "        out2 += self.bias2\n",
    "\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            assert alpha is not None\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                return out, (edge_index, alpha)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                return out, edge_index.set_value(alpha, layout='coo')\n",
    "        else:\n",
    "            return out , out2\n",
    "        return out, out2\n",
    "\n",
    "    def message(self, x_j: Tensor, x_i: Tensor, index: Tensor, ptr: OptTensor,\n",
    "                size_i: Optional[int]) -> Tensor:\n",
    "        x = x_i + x_j\n",
    "        alpha1 = (x[:, :, :self.out_channels] * self.att).sum(dim=-1)\n",
    "        alpha2 = (x[:, :, self.out_channels:] * self.att2).sum(dim=-1)\n",
    "        alpha1 = F.leaky_relu(alpha1 ,self.negative_slope )\n",
    "        alpha2 = F.leaky_relu(alpha2 ,self.negative_slope )\n",
    "        alpha1 = softmax(alpha1, index, ptr, size_i)\n",
    "        alpha2 = softmax(alpha2, index, ptr, size_i)\n",
    "        self._alpha1 = alpha1\n",
    "        self._alpha2 = alpha2\n",
    "        alpha1= F.dropout(alpha1, p=self.dropout, training=self.training)\n",
    "        alpha2= F.dropout(alpha2, p=self.dropout, training=self.training)\n",
    "        return torch.cat((x_j[:, :, :self.out_channels]* alpha2.unsqueeze(-1), x_j[:, :, self.out_channels: ]* alpha1.unsqueeze(-1)) ,dim = -1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3efe45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# ==== your GTATConv must be imported here ====\n",
    "# from your_file import GTATConv\n",
    "\n",
    "# ---- Create a tiny 4-node graph ----\n",
    "# edges: 0->1, 0->2, 1->2, 2->3, 3->0\n",
    "edge_index = torch.tensor([\n",
    "    [0, 0, 1, 2, 3],\n",
    "    [1, 2, 2, 3, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "num_nodes = 1212\n",
    "in_channels = 6\n",
    "out_channels = 6\n",
    "heads = 2\n",
    "topology_channels = 15\n",
    "\n",
    "x = torch.randn(num_nodes, in_channels)\n",
    "\n",
    "# topology must be per-node, not per-edge\n",
    "topology = torch.randn(num_nodes, topology_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e890ccce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1212, 15])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topology.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4636de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Create the layer ----\n",
    "conv = GTATConv(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    heads=heads,\n",
    "    topology_channels=topology_channels,\n",
    "    concat=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3145256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running message\n"
     ]
    }
   ],
   "source": [
    "out, out_topo = conv(\n",
    "    x=x,\n",
    "    edge_index=edge_index,\n",
    "    topology=topology\n",
    "    # drop return_attention_weights for now (the alpha bug)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25fe66da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0588,  0.1033, -0.4876,  ..., -0.9855,  0.3434, -0.0788],\n",
       "         [ 1.8255, -1.7424,  0.2190,  ..., -1.0102,  0.7253, -0.2346],\n",
       "         [ 0.5957, -0.8927, -0.1721,  ..., -1.0335,  0.0633, -0.3791],\n",
       "         ...,\n",
       "         [-0.2016, -0.1400, -0.2803,  ..., -0.1449, -0.0055, -0.6065],\n",
       "         [ 1.0063, -0.7693, -0.1875,  ..., -0.2388,  1.5637, -0.8959],\n",
       "         [-2.8115,  2.7337, -0.5551,  ...,  2.0506, -2.0814,  1.0832]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[ 0.2006, -0.7927,  0.4328,  ...,  0.1841, -0.6073, -0.3252],\n",
       "         [ 0.1825, -1.2819,  0.0595,  ..., -0.5152,  0.1847, -0.3861],\n",
       "         [-0.1153, -1.1041,  0.1479,  ..., -0.4474, -0.1341, -0.7020],\n",
       "         ...,\n",
       "         [ 0.7222,  1.9238, -0.4392,  ..., -0.2834, -1.7416,  0.7609],\n",
       "         [ 0.1801, -0.5501,  0.6030,  ...,  0.8499,  0.3245, -1.0969],\n",
       "         [-0.7432, -0.6839, -0.4942,  ...,  0.4759,  0.3428,  2.1522]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, out_topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa5d7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24053dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ce30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopoAugmentedQK(nn.Module):\n",
    "    \"\"\"\n",
    "    GTAT-style topology augmentation for Q/K, compatible with STGformer-style inputs.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x       : (B, T, N, C)   feature tensor (C = model_dim)\n",
    "    z_topo  : (N, d_t)       per-node topology embeddings (static over time & batch)\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    Q       : (B, T, N, C_q) augmented queries\n",
    "    K       : (B, T, N, C_k) augmented keys\n",
    "    V       : (B, T, N, C_v) values (feature-only, usually C_v = model_dim)\n",
    "\n",
    "    Typical use\n",
    "    -----------\n",
    "    - C_q = C_k = model_dim + topo_dim  (concat fusion)\n",
    "    - C_v = model_dim\n",
    "\n",
    "    Then you split Q, K, V into heads exactly as you already do in FastAttentionLayer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dim: int,\n",
    "        topo_dim: int,\n",
    "        num_heads: int,\n",
    "        topo_proj_dim: int = None,\n",
    "        fuse_mode: str = \"concat\",  # \"concat\" or \"add\"\n",
    "        init_beta: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert model_dim % num_heads == 0, \"model_dim must be divisible by num_heads\"\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # How big is the topology component we concatenate/add?\n",
    "        # If None, just match model_dim.\n",
    "        self.topo_proj_dim = topo_proj_dim or model_dim\n",
    "        self.fuse_mode = fuse_mode\n",
    "\n",
    "        # Feature branch: standard qkv\n",
    "        self.qkv_feat = nn.Linear(model_dim, 3 * model_dim)\n",
    "\n",
    "        # Topology branch: U, V projections\n",
    "        self.q_topo = nn.Linear(topo_dim, self.topo_proj_dim)\n",
    "        self.k_topo = nn.Linear(topo_dim, self.topo_proj_dim)\n",
    "\n",
    "        # Learnable scale for topo contribution (≈ sqrt(beta) in the derivation)\n",
    "        self.beta = nn.Parameter(torch.tensor(init_beta, dtype=torch.float32))\n",
    "\n",
    "        # Decide output dims for Q/K/V based on fusion mode\n",
    "        if self.fuse_mode == \"concat\":\n",
    "            self.q_dim = model_dim + self.topo_proj_dim\n",
    "            self.k_dim = model_dim + self.topo_proj_dim\n",
    "        elif self.fuse_mode == \"add\":\n",
    "            # add requires same dims\n",
    "            assert self.topo_proj_dim == model_dim, \\\n",
    "                \"For 'add' fusion, topo_proj_dim must equal model_dim\"\n",
    "            self.q_dim = model_dim\n",
    "            self.k_dim = model_dim\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fuse_mode: {self.fuse_mode}\")\n",
    "\n",
    "        self.v_dim = model_dim  # usually keep V feature-only\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,        # (B, T, N, C)\n",
    "        z_topo: torch.Tensor,   # (N, d_t)\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        B, T, N, C = x.shape\n",
    "        assert C == self.model_dim, \\\n",
    "            f\"x last dim ({C}) must equal model_dim ({self.model_dim})\"\n",
    "        assert z_topo.shape[0] == N, \\\n",
    "            \"z_topo must have one embedding per node (N).\"\n",
    "\n",
    "        # ---- Feature branch: q_feat, k_feat, v_feat ----\n",
    "        qkv_feat = self.qkv_feat(x)                       # (B, T, N, 3*C)\n",
    "        q_feat, k_feat, v_feat = qkv_feat.chunk(3, dim=-1)  # each (B, T, N, C)\n",
    "\n",
    "        # ---- Topology branch: broadcast + linear projections ----\n",
    "        # z_topo: (N, d_t) -> (1, 1, N, d_t) -> (B, T, N, d_t)\n",
    "        z_bt = z_topo.unsqueeze(0).unsqueeze(0).expand(B, T, -1, -1)  # (B, T, N, d_t)\n",
    "\n",
    "        q_topo = self.q_topo(z_bt)  # (B, T, N, topo_proj_dim)\n",
    "        k_topo = self.k_topo(z_bt)  # (B, T, N, topo_proj_dim)\n",
    "\n",
    "        # Scale topology contribution (≈ sqrt(beta))\n",
    "        # we keep beta as a single scalar for all heads; you could also make beta per-head\n",
    "        scale = self.beta\n",
    "\n",
    "        # ---- Fuse feature & topo branches into augmented Q/K ----\n",
    "        if self.fuse_mode == \"concat\":\n",
    "            # Q = [q_feat; sqrt(beta) * q_topo], same for K\n",
    "            Q = torch.cat([q_feat, scale * q_topo], dim=-1)  # (B, T, N, C + topo_proj_dim)\n",
    "            K = torch.cat([k_feat, scale * k_topo], dim=-1)\n",
    "        else:  # \"add\"\n",
    "            Q = q_feat + scale * q_topo\n",
    "            K = k_feat + scale * k_topo\n",
    "\n",
    "        V = v_feat  # values typically remain feature-only\n",
    "\n",
    "        return Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "151b761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopoAugmentedQK(nn.Module):\n",
    "    \"\"\"\n",
    "    GTAT-style topology augmentation for Q/K.\n",
    "\n",
    "    x      : (B, T, N, C)   feature tensor\n",
    "    z_topo : (N, d_t)       per-node topology embeddings\n",
    "\n",
    "    Returns:\n",
    "    Q, K, V: (B, T, N, C)   same last-dim as x (model_dim), so head_dim unchanged\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dim: int,\n",
    "        topo_dim: int,\n",
    "        num_heads: int,\n",
    "        topo_proj_dim: int = None,   # must equal model_dim for \"add\"\n",
    "        init_beta: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert model_dim % num_heads == 0, \"model_dim must be divisible by num_heads\"\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.topo_proj_dim = topo_proj_dim or model_dim\n",
    "        assert self.topo_proj_dim == model_dim, \\\n",
    "            \"For additive fusion, topo_proj_dim must equal model_dim\"\n",
    "\n",
    "        # Feature branch\n",
    "        self.qkv_feat = nn.Linear(model_dim, 3 * model_dim)\n",
    "\n",
    "        # Topology branch\n",
    "        self.q_topo = nn.Linear(topo_dim, self.topo_proj_dim)\n",
    "        self.k_topo = nn.Linear(topo_dim, self.topo_proj_dim)\n",
    "\n",
    "        # Learnable scale for topo contribution (≈ sqrt(beta))\n",
    "        self.beta = nn.Parameter(torch.tensor(init_beta, dtype=torch.float32))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,        # (B, T, N, C)\n",
    "        z_topo: torch.Tensor,   # (N, d_t)\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        B, T, N, C = x.shape\n",
    "        assert C == self.model_dim\n",
    "        assert z_topo.shape[0] == N, \"z_topo must have one embedding per node\"\n",
    "\n",
    "        # ---- feature Q/K/V ----\n",
    "        qkv_feat = self.qkv_feat(x)  # (B, T, N, 3*C)\n",
    "        q_feat, k_feat, v_feat = qkv_feat.chunk(3, dim=-1)  # each (B, T, N, C)\n",
    "\n",
    "        # ---- topo Q/K ----\n",
    "        # z_topo: (N, d_t) -> (1,1,N,d_t) -> (B,T,N,d_t)\n",
    "        z_bt = z_topo.unsqueeze(0).unsqueeze(0).expand(B, T, -1, -1)\n",
    "        q_topo = self.q_topo(z_bt)  # (B, T, N, C)\n",
    "        k_topo = self.k_topo(z_bt)  # (B, T, N, C)\n",
    "\n",
    "        # add topo contribution\n",
    "        scale = self.beta\n",
    "        Q = q_feat + scale * q_topo  # (B, T, N, C)\n",
    "        K = k_feat + scale * k_topo  # (B, T, N, C)\n",
    "        V = v_feat                   # (B, T, N, C)\n",
    "\n",
    "        return Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e77f7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, N, C = 32, 12, 1212, 16       # batch, time, nodes, model_dim\n",
    "d_t = 8                        # topo embedding dim\n",
    "H = 4                          # heads\n",
    "\n",
    "x = torch.randn(B, T, N, C)\n",
    "z_topo = torch.randn(N, d_t)   # per-node topo embeddings\n",
    "\n",
    "topo_qk = TopoAugmentedQK(\n",
    "    model_dim=C,\n",
    "    topo_dim=d_t,\n",
    "    num_heads=H,\n",
    "    topo_proj_dim=C,   # equals C because we use additive fusion\n",
    "    init_beta=0.5,\n",
    ")\n",
    "\n",
    "Q, K, V = topo_qk(x, z_topo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c11ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastAttentionLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads=8, topo_dim=8, kernel=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.head_dim = model_dim // num_heads\n",
    "\n",
    "        self.qkv = TopoAugmentedQK(model_dim, topo_dim, num_heads, model_dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(\n",
    "            2 * model_dim if kernel != 12 else model_dim, model_dim\n",
    "        )\n",
    "        # self.proj_in = nn.Conv2d(model_dim, model_dim, (1, kernel), 1, 0) if kernel > 1 else nn.Identity()\n",
    "        self.fast = 1\n",
    "\n",
    "    def forward(self, x, z, dim=0):\n",
    "        # z: topology embedding\n",
    "        query, key, value = self.qkv(x, z)\n",
    "        qs = torch.stack(torch.split(query, self.head_dim, dim=-1), dim=-2).flatten(\n",
    "            start_dim=dim, end_dim=dim + 1\n",
    "        )\n",
    "        ks = torch.stack(torch.split(key, self.head_dim, dim=-1), dim=-2).flatten(\n",
    "            start_dim=dim, end_dim=dim + 1\n",
    "        )\n",
    "        vs = torch.stack(torch.split(value, self.head_dim, dim=-1), dim=-2).flatten(\n",
    "            start_dim=dim, end_dim=dim + 1\n",
    "        )\n",
    "        if self.fast:\n",
    "            out_s = self.fast_attention(x, qs, ks, vs, dim=dim)\n",
    "        else:\n",
    "            out_s = self.normal_attention(x, qs, ks, vs, dim=dim)\n",
    "        if x.size(1) > 1:\n",
    "            qs = torch.stack(\n",
    "                torch.split(query.transpose(1, 2), self.head_dim, dim=-1), dim=-2\n",
    "            ).flatten(start_dim=dim, end_dim=dim + 1)\n",
    "            ks = torch.stack(\n",
    "                torch.split(key.transpose(1, 2), self.head_dim, dim=-1), dim=-2\n",
    "            ).flatten(start_dim=dim, end_dim=dim + 1)\n",
    "            vs = torch.stack(\n",
    "                torch.split(value.transpose(1, 2), self.head_dim, dim=-1), dim=-2\n",
    "            ).flatten(start_dim=dim, end_dim=dim + 1)\n",
    "            if self.fast:\n",
    "                out_t = self.fast_attention(\n",
    "                    x.transpose(1, 2), qs, ks, vs, dim=dim\n",
    "                ).transpose(1, 2)\n",
    "            else:\n",
    "                out_t = self.normal_attention(\n",
    "                    x.transpose(1, 2), qs, ks, vs, dim=dim\n",
    "                ).transpose(1, 2)\n",
    "            out = torch.concat([out_s, out_t], -1)\n",
    "            out = self.out_proj(out)\n",
    "        else:\n",
    "            out = self.out_proj(out_s)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def fast_attention(self, x, qs, ks, vs, dim=0):\n",
    "        qs = nn.functional.normalize(qs, dim=-1)\n",
    "        ks = nn.functional.normalize(ks, dim=-1)\n",
    "        N = qs.shape[1]\n",
    "        b, l = x.shape[dim : dim + 2]\n",
    "\n",
    "        # numerator\n",
    "        kvs = torch.einsum(\"blhm,blhd->bhmd\", ks, vs)\n",
    "        attention_num = torch.einsum(\"bnhm,bhmd->bnhd\", qs, kvs)  # [N, H, D]\n",
    "        attention_num += N * vs\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[1]]).to(ks.device)\n",
    "        ks_sum = torch.einsum(\"blhm,l->bhm\", ks, all_ones)\n",
    "        attention_normalizer = torch.einsum(\"bnhm,bhm->bnh\", qs, ks_sum)  # [N, H]\n",
    "\n",
    "        # attentive aggregated results\n",
    "        attention_normalizer = torch.unsqueeze(\n",
    "            attention_normalizer, len(attention_normalizer.shape)\n",
    "        )  # [N, H, 1]\n",
    "        attention_normalizer += torch.ones_like(attention_normalizer) * N\n",
    "        out = attention_num / attention_normalizer.clamp_min(1e-5)  # [N, H, D]\n",
    "        out = torch.unflatten(out, dim, (b, l)).flatten(start_dim=3)\n",
    "        return out\n",
    "\n",
    "    def normal_attention(self, x, qs, ks, vs, dim=0):\n",
    "        b, l = x.shape[dim : dim + 2]\n",
    "        qs, ks, vs = qs.transpose(1, 2), ks.transpose(1, 2), vs.transpose(1, 2)\n",
    "        x = (\n",
    "            torch.nn.functional.scaled_dot_product_attention(qs, ks, vs)\n",
    "            .transpose(-3, -2)\n",
    "            .flatten(start_dim=-2)\n",
    "        )\n",
    "        x = torch.unflatten(x, dim, (b, l)).flatten(start_dim=3)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d132180",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = FastAttentionLayer(model_dim=C, num_heads=H, topo_dim=d_t, kernel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77b7a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = attn(x, z_topo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223fe633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872c6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c82f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f12f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
