{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12cedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from typing import Optional, Tuple, Union\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, Size\n",
    "from torch_geometric.utils import (\n",
    "    add_self_loops,\n",
    "    remove_self_loops,\n",
    "    softmax,\n",
    ")\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "\n",
    "# PyG initialization helpers\n",
    "from torch_geometric.nn.inits import glorot, zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f8240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTATConv(MessagePassing):\n",
    "    _alpha: OptTensor\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, heads: int,\n",
    "                 topology_channels:int = 15,\n",
    "                 concat: bool = True, negative_slope: float = 0.2,\n",
    "                 dropout: float = 0., add_self_loops: bool = True,\n",
    "                 bias: bool = True, share_weights: bool = False, **kwargs):\n",
    "        super(GTATConv, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.topology_channels = topology_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.share_weights = share_weights\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias,\n",
    "                            weight_initializer='glorot')\n",
    "        \n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias,\n",
    "                                weight_initializer='glorot')\n",
    "        \n",
    "        \n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        self.att2 = Parameter(torch.Tensor(1, heads, self.topology_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha1 = None\n",
    "        self._alpha2 = None\n",
    "\n",
    "        self.bias2 =  Parameter(torch.Tensor(self.topology_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "        glorot(self.att)\n",
    "        glorot(self.att2)\n",
    "        zeros(self.bias)\n",
    "        zeros(self.bias2)\n",
    "\n",
    "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
    "                topology: Tensor,\n",
    "                size: Size = None, return_attention_weights: bool = None):\n",
    "        # type: (Union[Tensor, PairTensor], Tensor , Tensor, Size, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, Size, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, Size, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, Size, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            return_attention_weights (bool, optional): If set to :obj:`True`,\n",
    "                will additionally return the tuple\n",
    "                :obj:`(edge_index, attention_weights)`, holding the computed\n",
    "                attention weights for each edge. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l: OptTensor = None\n",
    "        x_r: OptTensor = None\n",
    "        if isinstance(x, Tensor):\n",
    "            assert x.dim() == 2\n",
    "            x_l = self.lin_l(x).view(-1, H, C)  #(N , heads, features)\n",
    "            if self.share_weights:\n",
    "                x_r = x_l\n",
    "            else:\n",
    "                x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "        topology = topology.unsqueeze(dim = 1)\n",
    "        topology = topology.repeat(1, self.heads, 1)\n",
    "        x_l = torch.cat((x_l,topology), dim = -1)\n",
    "        x_r = torch.cat((x_r,topology), dim = -1)\n",
    "\n",
    "        if self.add_self_loops:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                num_nodes = x_l.size(0)\n",
    "                if x_r is not None:\n",
    "                    num_nodes = min(num_nodes, x_r.size(0))\n",
    "                if size is not None:\n",
    "                    num_nodes = min(size[0], size[1])\n",
    "                edge_index, _ = remove_self_loops(edge_index)\n",
    "                edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                edge_index = set_diag(edge_index)\n",
    "\n",
    "        out_all = self.propagate(edge_index, x=(x_l, x_r), size=size)\n",
    "        out = out_all[ : , : , :self.out_channels ]\n",
    "        out2 = out_all[ : , : , self.out_channels:]\n",
    "        alpha1 = self._alpha1\n",
    "        self._alpha1 = None\n",
    "        alpha2 = self._alpha2\n",
    "        self._alpha2 = None\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.reshape(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        out2 = out2.mean(dim=1)\n",
    "        out2 += self.bias2\n",
    "\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            assert alpha is not None\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                return out, (edge_index, alpha)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                return out, edge_index.set_value(alpha, layout='coo')\n",
    "        else:\n",
    "            return out , out2\n",
    "        return out, out2\n",
    "\n",
    "    def message(self, x_j: Tensor, x_i: Tensor, index: Tensor, ptr: OptTensor,\n",
    "                size_i: Optional[int]) -> Tensor:\n",
    "        x = x_i + x_j\n",
    "        alpha1 = (x[:, :, :self.out_channels] * self.att).sum(dim=-1)\n",
    "        alpha2 = (x[:, :, self.out_channels:] * self.att2).sum(dim=-1)\n",
    "        alpha1 = F.leaky_relu(alpha1 ,self.negative_slope )\n",
    "        alpha2 = F.leaky_relu(alpha2 ,self.negative_slope )\n",
    "        alpha1 = softmax(alpha1, index, ptr, size_i)\n",
    "        alpha2 = softmax(alpha2, index, ptr, size_i)\n",
    "        self._alpha1 = alpha1\n",
    "        self._alpha2 = alpha2\n",
    "        alpha1= F.dropout(alpha1, p=self.dropout, training=self.training)\n",
    "        alpha2= F.dropout(alpha2, p=self.dropout, training=self.training)\n",
    "        return torch.cat((x_j[:, :, :self.out_channels]* alpha2.unsqueeze(-1), x_j[:, :, self.out_channels: ]* alpha1.unsqueeze(-1)) ,dim = -1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3efe45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# ==== your GTATConv must be imported here ====\n",
    "# from your_file import GTATConv\n",
    "\n",
    "# ---- Create a tiny 4-node graph ----\n",
    "# edges: 0->1, 0->2, 1->2, 2->3, 3->0\n",
    "edge_index = torch.tensor([\n",
    "    [0, 0, 1, 2, 3],\n",
    "    [1, 2, 2, 3, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "num_nodes = 1212\n",
    "in_channels = 6\n",
    "out_channels = 6\n",
    "heads = 2\n",
    "topology_channels = 15\n",
    "\n",
    "x = torch.randn(num_nodes, in_channels)\n",
    "\n",
    "# topology must be per-node, not per-edge\n",
    "topology = torch.randn(num_nodes, topology_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e890ccce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1212, 15])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topology.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4636de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Create the layer ----\n",
    "conv = GTATConv(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    heads=heads,\n",
    "    topology_channels=topology_channels,\n",
    "    concat=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3145256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running message\n"
     ]
    }
   ],
   "source": [
    "out, out_topo = conv(\n",
    "    x=x,\n",
    "    edge_index=edge_index,\n",
    "    topology=topology\n",
    "    # drop return_attention_weights for now (the alpha bug)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25fe66da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0588,  0.1033, -0.4876,  ..., -0.9855,  0.3434, -0.0788],\n",
       "         [ 1.8255, -1.7424,  0.2190,  ..., -1.0102,  0.7253, -0.2346],\n",
       "         [ 0.5957, -0.8927, -0.1721,  ..., -1.0335,  0.0633, -0.3791],\n",
       "         ...,\n",
       "         [-0.2016, -0.1400, -0.2803,  ..., -0.1449, -0.0055, -0.6065],\n",
       "         [ 1.0063, -0.7693, -0.1875,  ..., -0.2388,  1.5637, -0.8959],\n",
       "         [-2.8115,  2.7337, -0.5551,  ...,  2.0506, -2.0814,  1.0832]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[ 0.2006, -0.7927,  0.4328,  ...,  0.1841, -0.6073, -0.3252],\n",
       "         [ 0.1825, -1.2819,  0.0595,  ..., -0.5152,  0.1847, -0.3861],\n",
       "         [-0.1153, -1.1041,  0.1479,  ..., -0.4474, -0.1341, -0.7020],\n",
       "         ...,\n",
       "         [ 0.7222,  1.9238, -0.4392,  ..., -0.2834, -1.7416,  0.7609],\n",
       "         [ 0.1801, -0.5501,  0.6030,  ...,  0.8499,  0.3245, -1.0969],\n",
       "         [-0.7432, -0.6839, -0.4942,  ...,  0.4759,  0.3428,  2.1522]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, out_topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa5d7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
