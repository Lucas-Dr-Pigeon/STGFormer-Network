{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9bb73d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import yaml\n",
    "import json\n",
    "import sys\n",
    "import glob\n",
    "import copy\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb9c1228",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from lib.utils import (\n",
    "    MaskedMAELoss,\n",
    "    MaskedHuberLoss,\n",
    "    print_log,\n",
    "    seed_everything,\n",
    "    set_cpu_num,\n",
    "    masked_mae_loss,\n",
    "    CustomJSONEncoder,\n",
    ")\n",
    "from lib.metrics import RMSE_MAE_MAPE\n",
    "from lib.data_prepare import get_dataloaders_from_index_data, load_inrix_data_with_details\n",
    "from model.STGWformer_eval import STGWformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4f4408c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"STGWformer_INRIX_MANHATTAN\"\n",
    "MODE = 'test'\n",
    "DEVICE = 'cuda:1'\n",
    "SHIFT = True\n",
    "SCALER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b251b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference_graph(model):\n",
    "    graph = torch.matmul(model.adaptive_embedding, model.adaptive_embedding.transpose(1, 2))\n",
    "    graph = model.pooling(graph.transpose(0, 2)).transpose(0, 2)\n",
    "    graph = nn.functional.relu(graph)\n",
    "    graph = nn.functional.softmax(graph, dim=-1)\n",
    "    return graph\n",
    "\n",
    "def eval_model(model, valset_loader, criterion):\n",
    "    model.eval()\n",
    "    batch_loss_list = []\n",
    "    for x_batch, y_batch in valset_loader:\n",
    "        x_batch = x_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        out_batch = model(x_batch)\n",
    "        out_batch = SCALER.inverse_transform(out_batch)\n",
    "        loss = criterion(out_batch, y_batch)\n",
    "        batch_loss_list.append(loss.item())\n",
    "\n",
    "    return np.mean(batch_loss_list)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    y = []\n",
    "    out = []\n",
    "\n",
    "    for x_batch, y_batch in tqdm(loader):\n",
    "        x_batch = x_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "        out_batch = model(x_batch)\n",
    "        out_batch = SCALER.inverse_transform(out_batch)\n",
    "\n",
    "        out_batch = out_batch.cpu().numpy()\n",
    "        y_batch = y_batch.cpu().numpy()\n",
    "        out.append(out_batch)\n",
    "        y.append(y_batch)\n",
    "    _, _, num_nodes, _ = out_batch.shape\n",
    "    out = np.vstack(out).reshape(-1, 1, num_nodes)  # (samples, out_steps, num_nodes)\n",
    "    y = np.vstack(y).reshape(-1, 1, num_nodes)\n",
    "\n",
    "    return y, out\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model(model, testset_loader, log=None):\n",
    "    model.eval()\n",
    "    print_log(\"--------- Test ---------\", log=log)\n",
    "\n",
    "    start = time.time()\n",
    "    y_true, y_pred = predict(model, testset_loader)\n",
    "    end = time.time()\n",
    "\n",
    "    rmse_all, mae_all, mape_all = RMSE_MAE_MAPE(y_true, y_pred)\n",
    "    out_str = \"All Steps RMSE = %.5f, MAE = %.5f, MAPE = %.5f\\n\" % (\n",
    "        rmse_all,\n",
    "        mae_all,\n",
    "        mape_all,\n",
    "    )\n",
    "    # print (f\"--- y_true: {y_true.shape}  y_pred: {y_pred.shape} ---\")\n",
    "    out_steps = y_pred.shape[1]\n",
    "    for i in range(out_steps):\n",
    "        rmse, mae, mape = RMSE_MAE_MAPE(y_true[:, i, :], y_pred[:, i, :])\n",
    "        out_str += \"Step %d RMSE = %.5f, MAE = %.5f, MAPE = %.5f\\n\" % (\n",
    "            i + 1,\n",
    "            rmse,\n",
    "            mae,\n",
    "            mape,\n",
    "        )\n",
    "\n",
    "    print_log(out_str, log=log, end=\"\")\n",
    "    print_log(\"Inference time: %.2f s\" % (end - start), log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b75c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12278436",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.randint(0,1000)  # set random seed here\n",
    "seed_everything(seed)\n",
    "set_cpu_num(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8193b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_Weight = 'saved_models/STGWformer-STGWFORMER_INRIX_MANHATTAN-2025-10-07-02-18-59.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d61b3da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STGWFORMER_INRIX_MANHATTAN\n",
      "STGWFORMER_INRIX_MANHATTAN\n",
      "--- Building Sequences ---\n",
      "--- Scaling Sequences ---\n",
      "Trainset:\tx-(63057, 1212, 12, 1)\ty-(63057, 1212, 1, 1)\n",
      "Valset:  \tx-(21019, 1212, 12, 1)  \ty-(21019, 1212, 1, 1)\n",
      "Testset:\tx-(21020, 1212, 12, 1)\ty-(21020, 1212, 1, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = DATASET.upper()\n",
    "data_path = f\"../data/{dataset}\"\n",
    "\n",
    "model_name = STGWformer.__name__\n",
    "\n",
    "with open(f\"{model_name}.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "cfg = cfg[dataset]\n",
    "\n",
    "# ------------------------------- make log file ------------------------------ #\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "log_path = f\"../logs/\"\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)\n",
    "log = os.path.join(log_path, f\"{model_name}-{dataset}-{now}.log\")\n",
    "log = open(log, \"a\", encoding=\"utf-8\")\n",
    "log.seek(0)\n",
    "log.truncate()\n",
    "\n",
    "# ------------------------------- load dataset ------------------------------- #\n",
    "\n",
    "print_log(dataset, log=log)\n",
    "print_log(dataset, log=log)\n",
    "    \n",
    "(trainset_loader, valset_loader, testset_loader, SCALER, adj_mx, gdf, tmc) = (\n",
    "    load_inrix_data_with_details(\n",
    "        \"/home/dachuan/Productivities/Spectral GAT/NY/adj_manhattan.npy\",\n",
    "        \"/home/dachuan/Productivities/Spectral GAT/SPGAT/Data/speed_19_Manhattan_5min_py36\",\n",
    "        \"/home/dachuan/Productivities/Spectral GAT/NY/Manhattan_FinalVersion.shp\",\n",
    "        \"/home/dachuan/Productivities/Spectral GAT/NY/TMC_FinalVersion.csv\",\n",
    "        tod=cfg.get(\"time_of_day\"),\n",
    "        dow=cfg.get(\"day_of_week\"),\n",
    "        batch_size=cfg.get(\"batch_size\", 64),\n",
    "        history_seq_len=cfg.get(\"in_steps\"),\n",
    "        future_seq_len=cfg.get(\"out_steps\"),\n",
    "        log=log,\n",
    "        train_ratio=cfg.get(\"train_size\", 0.6),\n",
    "        valid_ratio=cfg.get(\"val_size\", 0.2),\n",
    "        shift=SHIFT,\n",
    "    )\n",
    ")\n",
    "\n",
    "print_log(log=log)\n",
    "supports = [torch.tensor(i).to(DEVICE) for i in adj_mx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "84af8a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dachuan/anaconda3/envs/deep/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- set loss, optimizer, scheduler ---------------------- #\n",
    "from functools import partial\n",
    "\n",
    "model = partial(STGWformer)\n",
    "model = model(**cfg[\"model_args\"])\n",
    "model = model.to(DEVICE)\n",
    "criterion = MaskedMAELoss()  # MaskedHuberLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=cfg[\"lr\"],\n",
    "    weight_decay=cfg.get(\"weight_decay\", 0),\n",
    "    eps=cfg.get(\"eps\", 1e-8),\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=cfg[\"milestones\"],\n",
    "    gamma=cfg.get(\"lr_decay_rate\"),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ff3adda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the latest model: ../saved_models/STGWformer-STGWFORMER_INRIX_MANHATTAN-2025-10-07-02-18-59.pt\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- set model saving path -------------------------- #\n",
    "save_path = f\"../saved_models/\"\n",
    "\n",
    "model_files = glob.glob(os.path.join(save_path, f\"{model_name}-{dataset}-*.pt\"))\n",
    "if not model_files:\n",
    "    raise ValueError(\"No saved model found for testing.\")\n",
    "latest_model = max(model_files, key=os.path.getctime)\n",
    "print_log(f\"Loading the latest model: {latest_model}\", log=log)\n",
    "model.load_state_dict(torch.load(latest_model))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "24079a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- STGWformer ---------\n",
      "{\n",
      "    \"num_nodes\": 1212,\n",
      "    \"in_steps\": 12,\n",
      "    \"out_steps\": 1,\n",
      "    \"train_size\": 0.6,\n",
      "    \"val_size\": 0.2,\n",
      "    \"time_of_day\": false,\n",
      "    \"day_of_week\": false,\n",
      "    \"lr\": 0.001,\n",
      "    \"weight_decay\": 0.0015,\n",
      "    \"milestones\": [\n",
      "        25,\n",
      "        45,\n",
      "        65\n",
      "    ],\n",
      "    \"lr_decay_rate\": 0.1,\n",
      "    \"batch_size\": 8,\n",
      "    \"max_epochs\": 300,\n",
      "    \"early_stop\": 30,\n",
      "    \"use_cl\": false,\n",
      "    \"cl_step_size\": 2500,\n",
      "    \"model_args\": {\n",
      "        \"num_nodes\": 1212,\n",
      "        \"in_steps\": 12,\n",
      "        \"out_steps\": 1,\n",
      "        \"steps_per_day\": 288,\n",
      "        \"input_dim\": 1,\n",
      "        \"output_dim\": 1,\n",
      "        \"input_embedding_dim\": 24,\n",
      "        \"tod_embedding_dim\": 0,\n",
      "        \"dow_embedding_dim\": 0,\n",
      "        \"adaptive_embedding_dim\": 12,\n",
      "        \"kernel_size\": [\n",
      "            1\n",
      "        ],\n",
      "        \"num_heads\": 4,\n",
      "        \"num_layers\": 6,\n",
      "        \"dropout\": 0.1,\n",
      "        \"dropout_a\": 0.35\n",
      "    }\n",
      "}\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "STGWformer                                    [8, 1, 1212, 1]           174,528\n",
      "├─Linear: 1-1                                 [8, 12, 1212, 24]         48\n",
      "├─Dropout: 1-2                                [8, 12, 1212, 12]         --\n",
      "├─Conv2d: 1-3                                 [8, 36, 1212, 12]         1,332\n",
      "├─AvgPool2d: 1-4                              [1212, 1212, 12]          --\n",
      "├─ModuleList: 1-5                             --                        --\n",
      "│    └─SelfAttentionLayerWavelet: 2-1         [8, 12, 1212, 36]         --\n",
      "│    │    └─SafeLayerNorm: 3-1                [8, 12, 1212, 36]         72\n",
      "│    │    └─GraphWaveletPropagate: 3-2        [8, 12, 1212, 36]         80\n",
      "│    │    └─SafeLayerNorm: 3-3                [8, 12, 1212, 36]         72\n",
      "│    │    └─ModuleList: 3-7                   --                        (recursive)\n",
      "│    │    └─ModuleList: 3-8                   --                        (recursive)\n",
      "│    │    └─SafeLayerNorm: 3-6                [8, 12, 1212, 36]         (recursive)\n",
      "│    │    └─ModuleList: 3-7                   --                        (recursive)\n",
      "│    │    └─ModuleList: 3-8                   --                        (recursive)\n",
      "│    │    └─Dropout: 3-9                      [8, 12, 1212, 36]         --\n",
      "│    │    └─SafeLayerNorm: 3-10               [8, 12, 1212, 36]         72\n",
      "│    │    └─SafeLayerNorm: 3-11               [8, 12, 1212, 36]         72\n",
      "│    │    └─Mlp: 3-12                         [8, 12, 1212, 36]         5,292\n",
      "│    │    └─Dropout: 3-13                     [8, 12, 1212, 36]         --\n",
      "│    │    └─SafeLayerNorm: 3-14               [8, 12, 1212, 36]         72\n",
      "├─Linear: 1-6                                 [8, 1212, 36]             15,588\n",
      "├─ModuleList: 1-7                             --                        --\n",
      "│    └─Mlp: 2-2                               [8, 1212, 36]             --\n",
      "│    │    └─Linear: 3-15                      [8, 1212, 72]             2,664\n",
      "│    │    └─ReLU: 3-16                        [8, 1212, 72]             --\n",
      "│    │    └─Dropout: 3-17                     [8, 1212, 72]             --\n",
      "│    │    └─Identity: 3-18                    [8, 1212, 72]             --\n",
      "│    │    └─Linear: 3-19                      [8, 1212, 36]             2,628\n",
      "│    │    └─Dropout: 3-20                     [8, 1212, 36]             --\n",
      "│    └─Mlp: 2-3                               [8, 1212, 36]             --\n",
      "│    │    └─Linear: 3-21                      [8, 1212, 72]             2,664\n",
      "│    │    └─ReLU: 3-22                        [8, 1212, 72]             --\n",
      "│    │    └─Dropout: 3-23                     [8, 1212, 72]             --\n",
      "│    │    └─Identity: 3-24                    [8, 1212, 72]             --\n",
      "│    │    └─Linear: 3-25                      [8, 1212, 36]             2,628\n",
      "│    │    └─Dropout: 3-26                     [8, 1212, 36]             --\n",
      "│    └─Mlp: 2-4                               [8, 1212, 36]             --\n",
      "│    │    └─Linear: 3-27                      [8, 1212, 72]             2,664\n",
      "│    │    └─ReLU: 3-28                        [8, 1212, 72]             --\n",
      "│    │    └─Dropout: 3-29                     [8, 1212, 72]             --\n",
      "│    │    └─Identity: 3-30                    [8, 1212, 72]             --\n",
      "│    │    └─Linear: 3-31                      [8, 1212, 36]             2,628\n",
      "│    │    └─Dropout: 3-32                     [8, 1212, 36]             --\n",
      "│    └─Mlp: 2-5                               [8, 1212, 36]             --\n",
      "│    │    └─Linear: 3-33                      [8, 1212, 72]             2,664\n",
      "│    │    └─ReLU: 3-34                        [8, 1212, 72]             --\n",
      "│    │    └─Dropout: 3-35                     [8, 1212, 72]             --\n",
      "│    │    └─Identity: 3-36                    [8, 1212, 72]             --\n",
      "│    │    └─Linear: 3-37                      [8, 1212, 36]             2,628\n",
      "│    │    └─Dropout: 3-38                     [8, 1212, 36]             --\n",
      "│    └─Mlp: 2-6                               [8, 1212, 36]             --\n",
      "│    │    └─Linear: 3-39                      [8, 1212, 72]             2,664\n",
      "│    │    └─ReLU: 3-40                        [8, 1212, 72]             --\n",
      "│    │    └─Dropout: 3-41                     [8, 1212, 72]             --\n",
      "│    │    └─Identity: 3-42                    [8, 1212, 72]             --\n",
      "│    │    └─Linear: 3-43                      [8, 1212, 36]             2,628\n",
      "│    │    └─Dropout: 3-44                     [8, 1212, 36]             --\n",
      "│    └─Mlp: 2-7                               [8, 1212, 36]             --\n",
      "│    │    └─Linear: 3-45                      [8, 1212, 72]             2,664\n",
      "│    │    └─ReLU: 3-46                        [8, 1212, 72]             --\n",
      "│    │    └─Dropout: 3-47                     [8, 1212, 72]             --\n",
      "│    │    └─Identity: 3-48                    [8, 1212, 72]             --\n",
      "│    │    └─Linear: 3-49                      [8, 1212, 36]             2,628\n",
      "│    │    └─Dropout: 3-50                     [8, 1212, 36]             --\n",
      "├─Linear: 1-8                                 [8, 1212, 1]              37\n",
      "===============================================================================================\n",
      "Total params: 244,713\n",
      "Trainable params: 244,713\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 155.53\n",
      "===============================================================================================\n",
      "Input size (MB): 0.47\n",
      "Forward/backward pass size (MB): 745.66\n",
      "Params size (MB): 0.28\n",
      "Estimated Total Size (MB): 746.41\n",
      "===============================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- print model structure -------------------------- #\n",
    "\n",
    "print_log(\"---------\", model_name, \"---------\", log=log)\n",
    "print_log(\n",
    "    json.dumps(cfg, ensure_ascii=False, indent=4, cls=CustomJSONEncoder), log=log\n",
    ")\n",
    "print_log(\n",
    "    summary(\n",
    "        model,\n",
    "        [\n",
    "            cfg[\"batch_size\"],\n",
    "            cfg[\"in_steps\"],\n",
    "            cfg[\"num_nodes\"],\n",
    "            next(iter(trainset_loader))[0].shape[-1],\n",
    "        ],\n",
    "        verbose=0,  # avoid print twice\n",
    "        device=DEVICE,\n",
    "    ),\n",
    "    log=log,\n",
    ")\n",
    "print_log(log=log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "10997f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Test ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2627/2627 [01:31<00:00, 28.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Steps RMSE = 5.18845, MAE = 2.50357, MAPE = 18.29603\n",
      "Step 1 RMSE = 5.18845, MAE = 2.50357, MAPE = 18.29603\n",
      "Inference time: 91.99 s\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- evaluate model performance --------------------------- #\n",
    "test_model(model, testset_loader, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "27718ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- inference graph --------------------------- #\n",
    "graph = inference_graph(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7057535",
   "metadata": {},
   "source": [
    "### Inference wavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "931097f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "@torch.no_grad()\n",
    "def average_U_instance(model, loader, steps=8):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    # 1) Find the noflayer instance actually used\n",
    "    wave = model.attn_layers_s[0].locals.wavelet_layer  # your noflayer instance\n",
    "\n",
    "    # 2) Keep originals\n",
    "    orig_attention = wave.attention\n",
    "\n",
    "    # 3) Tape + tapped attention\n",
    "    TAPE = {'Ulist': []}\n",
    "    def tapped_attention(self, x_BTNC, A_TNN):\n",
    "        U, P, A_BTNN = orig_attention(x_BTNC, A_TNN)  # call the real thing\n",
    "        TAPE['Ulist'].append(U.detach().cpu())\n",
    "        return U, P, A_BTNN\n",
    "\n",
    "    # 4) Monkey-patch the INSTANCE (bind with MethodType)\n",
    "    wave.attention = types.MethodType(tapped_attention, wave)\n",
    "\n",
    "    U_acc, n = None, 0\n",
    "    it = iter(loader)\n",
    "    try:\n",
    "        for _ in range(steps):\n",
    "            batch = next(it)\n",
    "            batch_x = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "            batch_x = batch_x.to(device)\n",
    "\n",
    "            _ = model(batch_x)  # this will now push U’s into TAPE\n",
    "\n",
    "            for U in TAPE['Ulist']:\n",
    "                Umean = U.mean(0).mean(0).numpy()  # (T,N,N) → (N,N)\n",
    "                U_acc = Umean if U_acc is None else U_acc + Umean\n",
    "                n += 1\n",
    "            TAPE['Ulist'].clear()\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    finally:\n",
    "        # 5) Restore original method no matter what\n",
    "        wave.attention = orig_attention\n",
    "\n",
    "    if n == 0:\n",
    "        raise RuntimeError(\"average_U_instance: never captured any U. \"\n",
    "                           \"Check that the forward actually calls wavelet_layer.attention.\")\n",
    "    return U_acc / n  # Ubar (N,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4171ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- inference wavelets --------------------------- #\n",
    "def get_coe_cheb(wave):\n",
    "    coe  = torch.sigmoid(wave.temp).detach().cpu().numpy()\n",
    "    cheb = torch.sigmoid(wave.cheb).detach().cpu().numpy()\n",
    "    if wave.alpha_ is not None:  # override if alpha_ is set\n",
    "        a1 = float(wave.alpha_)\n",
    "        a2 = float(wave.alpha_)\n",
    "    else:\n",
    "        a1 = float(coe[1])\n",
    "        a2 = float(coe[2])\n",
    "    return a1, a2, cheb\n",
    "\n",
    "def compute_betas(K, a1, a2, cheb, r0):\n",
    "    betas = []\n",
    "    r_s = float(r0)\n",
    "    for s in range(K):  # s=0..K-1 => k=s+1\n",
    "        w_s = 1.0 - (1.0 - a1) * r_s\n",
    "        beta = (1.0 - a2) * (a2 ** (K-1 - s)) * w_s\n",
    "        betas.append(beta)\n",
    "        if s < K-1 and len(cheb) > s:\n",
    "            r_s *= float(cheb[s])\n",
    "    return np.array(betas, dtype=float)  # shape (K,)\n",
    "\n",
    "def poly_operator(Ubar, betas, symmetrize=True):\n",
    "    U = 0.5*(Ubar+Ubar.T) if symmetrize else Ubar\n",
    "    N = U.shape[0]\n",
    "    M = np.zeros((N,N), dtype=float)\n",
    "    Uk = np.eye(N)\n",
    "    for k, beta_k in enumerate(betas, start=1):\n",
    "        Uk = Uk @ U\n",
    "        M += beta_k * Uk\n",
    "    return M\n",
    "\n",
    "def centered_poly_operator(Ubar, betas):\n",
    "    U = 0.5*(Ubar+Ubar.T)\n",
    "    # stationary distribution ~ principal eigenvector (normalize to sum 1)\n",
    "    vals, vecs = np.linalg.eigh(U)\n",
    "    v = vecs[:, -1]; pi = np.abs(v); pi = pi / pi.sum()\n",
    "    J = np.ones((U.shape[0],1)) @ pi[None,:]  # 1 * pi^T\n",
    "\n",
    "    M = np.zeros_like(U)\n",
    "    Uk = np.eye(U.shape[0])\n",
    "    for k, beta in enumerate(betas, start=1):\n",
    "        Uk = Uk @ U\n",
    "        M += beta * (Uk - J)          # <-- remove rank-1 limit each hop\n",
    "    return M\n",
    "\n",
    "def poly_operator_unsym(Ubar, betas):\n",
    "    M = np.zeros_like(Ubar)\n",
    "    Uk = np.eye(Ubar.shape[0])\n",
    "    for k, beta in enumerate(betas, start=1):\n",
    "        Uk = Uk @ Ubar\n",
    "        M += beta * Uk\n",
    "    return M\n",
    "\n",
    "def row_topk(U, k=8):\n",
    "    U2 = U.copy()\n",
    "    idx = np.argsort(U2, axis=1)[:, :-k]\n",
    "    U2[np.arange(U2.shape[0])[:,None], idx] = 0.0\n",
    "    # renormalize rows\n",
    "    rs = U2.sum(axis=1, keepdims=True); rs[rs==0]=1\n",
    "    return U2/rs\n",
    "\n",
    "def row_normalize(M):\n",
    "    M_pos = np.maximum(M, 0.0)\n",
    "    rs = M_pos.sum(axis=1, keepdims=True)\n",
    "    rs[rs==0] = 1.0\n",
    "    return M_pos / rs\n",
    "\n",
    "def center_and_spectral_norm(M):\n",
    "    # Stationary via principal eigenvector of symmetric part (stable)\n",
    "    S = 0.5*(M+M.T)\n",
    "    vals, vecs = np.linalg.eigh(S)\n",
    "    pi = np.abs(vecs[:, -1]); pi = pi / pi.sum()\n",
    "    J = np.ones((M.shape[0],1)) @ pi[None,:]\n",
    "    R = M - J\n",
    "    # spectral norm (≈ largest singular value)\n",
    "    smax = np.linalg.svd(R, compute_uv=False)[0]\n",
    "    return R / (smax if smax>0 else 1.0)\n",
    "\n",
    "def min_max_normalize(M):\n",
    "    m_min = np.min(M)\n",
    "    m_max = np.max(M)\n",
    "    return (M-m_min)/(m_max-m_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "233d3bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave = model.attn_layers_s[0].locals.wavelet_layer\n",
    "a1, a2, cheb = get_coe_cheb(wave)\n",
    "betas = compute_betas(1, a1, a2, cheb, 0.5)\n",
    "\n",
    "Ubar = average_U_instance(model, trainset_loader, steps=8)\n",
    "m_indices = poly_operator_unsym(Ubar, betas)\n",
    "m_indices = min_max_normalize(m_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dd3584d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1468928"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m_indices[m_indices > 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f8e64d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m_indices[m_indices < 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cb815a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_U_stats(model, loader, steps=1):\n",
    "    device = next(model.parameters()).device\n",
    "    wave = model.attn_layers_s[0].locals.wavelet_layer  # noflayer instance\n",
    "    orig_attn = wave.attention\n",
    "    stats = []\n",
    "\n",
    "    def tapped_attn(self, x, A):\n",
    "        # --- copy your attention, but collect e and mask stats\n",
    "        a1 = self.a[:self.in_features,:]; a2 = self.a[self.in_features:,:]\n",
    "        feat_1 = torch.matmul(x, a1); feat_2 = torch.matmul(x, a2)\n",
    "        e = self.leakyrelu(feat_1 + feat_2.transpose(-2,-1))  # (B,T,N,N)\n",
    "\n",
    "        A_BTNN = A.unsqueeze(0).expand(x.size(0),-1,-1,-1)\n",
    "        mask = (A_BTNN > 0)\n",
    "\n",
    "        neg_inf = torch.finfo(e.dtype).min\n",
    "        e_masked = torch.where(mask, e, e.new_full((), neg_inf))\n",
    "\n",
    "        # --- collect per-row std of logits (variance drives softmax sharpness)\n",
    "        with torch.no_grad():\n",
    "            em = e_masked.clone()\n",
    "            em[~mask] = 0\n",
    "            # std across neighbors j\n",
    "            row_std = em.float().std(dim=-1, unbiased=False).mean().item()\n",
    "            mask_density = mask.float().mean().item()\n",
    "            stats.append((row_std, mask_density))\n",
    "\n",
    "        U = torch.softmax(e_masked, dim=-1)\n",
    "        return U, 0.5*U, A_BTNN\n",
    "\n",
    "    wave.attention = types.MethodType(tapped_attn, wave)\n",
    "    try:\n",
    "        it = iter(loader)\n",
    "        for _ in range(steps):\n",
    "            bx = next(it)[0].to(device)\n",
    "            _ = model(bx)\n",
    "    finally:\n",
    "        wave.attention = orig_attn\n",
    "\n",
    "    row_std_mean = float(np.mean([s for s,_ in stats]))\n",
    "    mask_density_mean = float(np.mean([d for _,d in stats]))\n",
    "    print(f\"[U debug] mean row-logit std: {row_std_mean:.6f}  |  mask density: {mask_density_mean:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a073335b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[U debug] mean row-logit std: 0.000000  |  mask density: 1.000\n"
     ]
    }
   ],
   "source": [
    "debug_U_stats(model, trainset_loader, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a10b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
