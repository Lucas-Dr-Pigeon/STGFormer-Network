{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb91090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37ce00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_row_softmax(logits: torch.Tensor,\n",
    "                       mask: torch.Tensor,\n",
    "                       temperature: float = 1.0,\n",
    "                       fallback_self_loop: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform row-wise softmax only over entries where mask==1; others get prob 0.\n",
    "    If a row has no allowed entries and fallback_self_loop=True, we set the diagonal\n",
    "    mask to 1 for that row before softmax (so it becomes a self-loop).\n",
    "    Shapes: logits, mask -> (B,N,N) or (1,N,N) for mask (will broadcast to B).\n",
    "    \"\"\"\n",
    "    if mask is None:\n",
    "        # standard row-softmax over all columns\n",
    "        return F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "    # Broadcast mask to (B,N,N)\n",
    "    m = mask.bool()\n",
    "    if m.dim() == 3 and logits.dim() == 3 and m.size(0) == 1:\n",
    "        m = m.expand(logits.size(0), -1, -1)\n",
    "\n",
    "    B, N, _ = logits.shape\n",
    "    # Optionally ensure at least one allowed entry per row (to avoid all -inf → NaN)\n",
    "    if fallback_self_loop:\n",
    "        row_has_any = m.any(dim=-1)  # (B,N)\n",
    "        if (~row_has_any).any():\n",
    "            eye = torch.eye(N, device=logits.device, dtype=m.dtype).bool().unsqueeze(0)\n",
    "            m = torch.where(~row_has_any.unsqueeze(-1) & eye, True, m)\n",
    "\n",
    "    # Set disallowed positions to -inf so softmax gives ~0 there\n",
    "    neg_inf = torch.finfo(logits.dtype).min\n",
    "    masked_logits = torch.where(m, logits / temperature, neg_inf)\n",
    "\n",
    "    # Numerically stable softmax: subtract row-max over allowed entries\n",
    "    row_max = masked_logits.max(dim=-1, keepdim=True).values  # (B,N,1)\n",
    "    exps = torch.exp(masked_logits - row_max) * m  # zero where disallowed\n",
    "    denom = exps.sum(dim=-1, keepdim=True).clamp_min(1e-12)\n",
    "    probs = exps / denom\n",
    "    return probs  # (B,N,N), rows sum to 1 over allowed entries only\n",
    "\n",
    "\n",
    "class TemporalNodeEncoder(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int = 64, out_dim: int = 64, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=in_dim, hidden_size=hidden,\n",
    "                          batch_first=True, bidirectional=bidirectional)\n",
    "        self.proj = nn.Linear(hidden*(2 if bidirectional else 1), out_dim)\n",
    "\n",
    "    def forward(self, x_BTNC: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, N, C = x_BTNC.shape\n",
    "        x = x_BTNC.permute(0, 2, 1, 3).reshape(B*N, T, C)   # (B*N,T,C)\n",
    "        h, _ = self.gru(x)\n",
    "        h = self.proj(h[:, -1])                             # (B*N,H)\n",
    "        return h.view(B, N, -1)                             # (B,N,H)\n",
    "\n",
    "\n",
    "class EdgeScorer(nn.Module):\n",
    "    def __init__(self, node_dim: int, edge_feat_dim: int = 0, hidden: int = 128):\n",
    "        super().__init__()\n",
    "        in_d = 4*node_dim + edge_feat_dim      # [hi, hj, hi-hj, hi*hj, (+ e_ij)]\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_d, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, H_BNH: torch.Tensor, Efeat_BNNE: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        B, N, H = H_BNH.shape\n",
    "        hi = H_BNH[:, :, None, :].expand(B, N, N, H)\n",
    "        hj = H_BNH[:, None, :, :].expand(B, N, N, H)\n",
    "        feats = [hi, hj, hi - hj, hi * hj]\n",
    "        if Efeat_BNNE is not None:\n",
    "            feats.append(Efeat_BNNE)\n",
    "        Z = torch.cat(feats, dim=-1)\n",
    "        return self.mlp(Z).squeeze(-1)  # (B,N,N)\n",
    "\n",
    "\n",
    "class PropagationGraphLearner(nn.Module):\n",
    "    \"\"\"\n",
    "    Learns a soft, directed propagation graph G (B,N,N) with masked row-softmax.\n",
    "    Self-loops are allowed if the mask diagonal is 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, node_dim: int = 64, edge_hidden: int = 128,\n",
    "                 scorer_edge_feat_dim: int = 0, temperature: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.encoder = TemporalNodeEncoder(in_dim, hidden=node_dim, out_dim=node_dim)\n",
    "        self.scorer  = EdgeScorer(node_dim, edge_feat_dim=scorer_edge_feat_dim, hidden=edge_hidden)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, x_BTNC: torch.Tensor,\n",
    "                mask_M: torch.Tensor,                  # (1 or B, N, N) with 1=allowed (neighbors)\n",
    "                edge_feats: torch.Tensor | None = None,\n",
    "                fallback_self_loop: bool = True):\n",
    "        H = self.encoder(x_BTNC)                        # (B,N,H)\n",
    "        logits = self.scorer(H, edge_feats)             # (B,N,N), includes self logits\n",
    "        soft = masked_row_softmax(logits, mask_M, self.temperature, fallback_self_loop)\n",
    "        return soft  # (B,N,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72e9865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, N, C = 4, 12, 300, 8\n",
    "x = torch.randn(B, T, N, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98fd16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical mask M (endpoint proximity, turn restrictions …)\n",
    "# shape can be (1,N,N) to broadcast across batch\n",
    "M = torch.randint(0, 2, (1, N, N)).bool()\n",
    "M[:, torch.arange(N), torch.arange(N)] = False  # forbid self loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d7bbe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = PropagationGraphLearner(\n",
    "    in_dim=C,\n",
    "    node_dim=64,\n",
    "    edge_hidden=128,\n",
    "    scorer_edge_feat_dim=0,     # set to E.shape[-1] if you pass edge features\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ac41e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_P = learner(x, mask_M=M, edge_feats=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a5426fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  ...,  True, False,  True],\n",
       "         [ True, False,  True,  ...,  True, False, False],\n",
       "         [False, False, False,  ...,  True, False,  True],\n",
       "         ...,\n",
       "         [ True,  True, False,  ..., False,  True,  True],\n",
       "         [ True, False,  True,  ...,  True, False,  True],\n",
       "         [False, False,  True,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cba3637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0073, 0.0072,  ..., 0.0073, 0.0000, 0.0073],\n",
       "         [0.0070, 0.0000, 0.0070,  ..., 0.0071, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0065],\n",
       "         ...,\n",
       "         [0.0067, 0.0069, 0.0000,  ..., 0.0000, 0.0066, 0.0069],\n",
       "         [0.0066, 0.0000, 0.0066,  ..., 0.0066, 0.0000, 0.0066],\n",
       "         [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0074, 0.0074,  ..., 0.0073, 0.0000, 0.0073],\n",
       "         [0.0071, 0.0000, 0.0072,  ..., 0.0070, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0065],\n",
       "         ...,\n",
       "         [0.0068, 0.0069, 0.0000,  ..., 0.0000, 0.0068, 0.0069],\n",
       "         [0.0067, 0.0000, 0.0067,  ..., 0.0066, 0.0000, 0.0067],\n",
       "         [0.0000, 0.0000, 0.0068,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0072, 0.0073,  ..., 0.0072, 0.0000, 0.0073],\n",
       "         [0.0070, 0.0000, 0.0071,  ..., 0.0070, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0064],\n",
       "         ...,\n",
       "         [0.0068, 0.0068, 0.0000,  ..., 0.0000, 0.0068, 0.0069],\n",
       "         [0.0066, 0.0000, 0.0066,  ..., 0.0065, 0.0000, 0.0066],\n",
       "         [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0072, 0.0073,  ..., 0.0072, 0.0000, 0.0072],\n",
       "         [0.0071, 0.0000, 0.0070,  ..., 0.0071, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0065],\n",
       "         ...,\n",
       "         [0.0068, 0.0068, 0.0000,  ..., 0.0000, 0.0068, 0.0068],\n",
       "         [0.0067, 0.0000, 0.0067,  ..., 0.0067, 0.0000, 0.0067],\n",
       "         [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81ccea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
